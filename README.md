# House-Prices-Advanced-Regression-Techniques

# House Prices Prediction: Advanced Regression Techniques

## Overview
This project aims to predict house prices using various regression techniques, focusing on building a robust machine learning pipeline. The dataset used in this project comes from the Kaggle competition "House Prices: Advanced Regression Techniques."

- **Project Goal**: Predict the sale prices of houses based on various features.
- **Dataset**: [House Prices: Advanced Regression Techniques](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data)
  
## Project Lifecycle
The project is divided into multiple stages:
1. **Exploratory Data Analysis (EDA)**: Understanding the data, identifying missing values, analyzing distributions, and exploring relationships between features.
2. **Feature Engineering**: Handling missing values, creating new features from temporal data, and standardizing categorical variables.
3. **Feature Selection**: Using Lasso Regression to select relevant features and reduce dimensionality.
4. **Model Building**: Implementing and training regression models.
5. **Model Evaluation**: Assessing model performance using various metrics and fine-tuning for better results.

## Key Steps
- **Data Analysis**: Explored the data to find patterns, visualize distributions, and identify outliers.
- **Feature Engineering**: Addressed missing values, transformed temporal features, and standardized categorical data.
- **Feature Selection**: Applied Lasso Regression to identify and retain important features for the predictive model.
- **Model Training**: Developed regression models, focusing on reducing overfitting and improving accuracy.
- **Evaluation**: Used mean squared error and RÂ² scores to evaluate model performance.

## Tools & Technologies
- Python, Pandas, NumPy
- Scikit-Learn (for feature selection and model building)
- Matplotlib, Seaborn (for data visualization)

## How to Use
1. Clone the repository.
2. Download the dataset from Kaggle and place it in the root folder.
3. Run the Jupyter notebooks provided for each phase (EDA, Feature Engineering, Feature Selection).
4. Train and evaluate the models using the provided scripts.

## Results
- Achieved a better understanding of the data through detailed EDA.
- Reduced the number of features using Lasso regression, improving the model's interpretability.
- Trained and fine-tuned models to achieve optimal performance in predicting house prices.

## Acknowledgments
- Kaggle for providing the dataset and competition platform.
